apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: vllm-metric-alerts
  namespace: m3
  labels:
    openshift.io/user-monitoring: "true" 
    openshift.io/prometheus-rule-evaluation-scope: "leaf-prometheus"
    role: alert-rules
spec:
  groups:
  - name: dummy-alerts
    interval: 0s
    rules:
    - alert: VLLMDummyServiceInfo
      expr: |
        # This will fire for every instance of vllm:request_success_total that exists
        rate(vllm:request_success_total[1m]) >= 0
      for: 0s 
      labels:
        severity: info
        test_alert: "true"
        expr: |
          # This will fire for every instance of vllm:request_success_total that exists
          rate(vllm:request_success_total[1m]) >= 0
        for: 0s 
      annotations:
        summary: "DUMMY ALERT: Information about vLLM service {{ $labels.service }} in namespace {{ $labels.namespace }}"
        description: |
          This test alert provides current context about a vLLM service instance.
          It is always firing for testing purposes and does not indicate an issue.
          Model Name: {{ $labels.model_name }}.
          Affected Pod: {{ $labels.pod }}.
          Current Rate of Successful Requests (1m): {{ $value | printf "%.2f" }} req/s

    - alert: VLLMDummyAlwaysFiring
      expr: |
        vector(1)
      for: 0s 
      labels:
        severity: info
        test_alert: "true"
        expr: |
          vector(1)
        for: 0s 
      annotations:
        summary: "DUMMY ALERT: This alert is always firing for testing purposes."
        description: |
          This is a test alert. It is configured to always be in a firing state.
          It does not indicate any actual issue with vLLM services.
          Please silence or remove this alert after testing is complete.
          Current value: {{ $value }}

  - name: vllm-metric-alerts
    interval: 15s
    rules:

    - alert: VLLMHighAbortedRequestRate
      expr: |
        rate(vllm:num_aborted_requests[5m]) / rate(vllm:num_total_requests[5m]) > 0.10 and rate(vllm:num_total_requests[5m]) > 0
      for: 5m
      labels:
        severity: critical
        expr: |
          rate(vllm:num_aborted_requests[5m]) / rate(vllm:num_total_requests[5m]) > 0.10 and rate(vllm:num_total_requests[5m]) > 0
        for: 5m
      annotations:
        summary: "High rate of aborted requests for vLLM service {{ $labels.service }} in namespace {{ $labels.namespace }}"
        description: | 
          More than 10% of total requests are being aborted over a 5 minute period.
          Rate of Aborted Requests: {{ $value | printf "%.2f" }}%
          Model Name: {{ $labels.model_name }}.
          Affected Pod: {{ $labels.pod }}.

    - alert: VLLMHighP95Latency
      expr: |
        histogram_quantile(0.95, sum by (instance, job, namespace, service, model_name, pod, le) (rate(vllm:e2e_request_latency_seconds_bucket[5m]))) > 5
      for: 5m
      labels:
        severity: critical
        expr: |
          histogram_quantile(0.95, sum by (instance, job, namespace, service, model_name, pod, le) (rate(vllm:e2e_request_latency_seconds_bucket[5m]))) > 5
        for: 5m
      annotations:
        summary: "High P95 end-to-end request latency for vLLM service {{ $labels.service }} in namespace {{ $labels.namespace }}"
        description: |
          The 95th percentile of end-to-end request latency is consistently above 5 seconds over a 5-minute window.
          P95 Latency: {{ $value | printf "%.2f" }}s
          Model Name: {{ $labels.model_name }}.
          Affected Pod: {{ $labels.pod }}.

    - alert: VLLMLowRequestSuccessRate
      expr: |
        (sum by (instance, job, namespace, service, model_name, pod) (rate(vllm:request_success_total[5m]))) / (sum by (instance, job, namespace, service, model_name, pod) (rate(vllm:num_total_requests[5m]))) < 0.95 and (sum by (instance, job, namespace, service, model_name, pod) (rate(vllm:num_total_requests[5m]))) > 0
      for: 5m
      labels:
        severity: critical
        expr: |
          (sum by (instance, job, namespace, service, model_name, pod) (rate(vllm:request_success_total[5m]))) / (sum by (instance, job, namespace, service, model_name, pod) (rate(vllm:num_total_requests[5m]))) < 0.95 and (sum by (instance, job, namespace, service, model_name, pod) (rate(vllm:num_total_requests[5m]))) > 0
        for: 5m
      annotations:
        summary: "Low request success rate for vLLM service {{ $labels.service }} in namespace {{ $labels.namespace }}"
        description: |
          The success rate of requests has fallen below 95% over a 5-minute window.
          Success Rate: {{ $value | printf "%.2f" }}%
          Model Name: {{ $labels.model_name }}.
          Affected Pod: {{ $labels.pod }}.

    - alert: VLLMHighAverageInferenceTime
      expr: |
        rate(vllm:request_inference_time_seconds_sum[5m]) / rate(vllm:request_inference_time_seconds_count[5m]) > 2
      for: 5m
      labels:
        severity: warning
        expr: |
          rate(vllm:request_inference_time_seconds_sum[5m]) / rate(vllm:request_inference_time_seconds_count[5m]) > 2
        for: 5m
      annotations:
        summary: "High average inference time for vLLM service {{ $labels.service }} in namespace {{ $labels.namespace }}"
        description: |
          The average inference time per request is consistently above 2 seconds over a 5-minute window.
          Average Inference Time: {{ $value | printf "%.2f" }}s
          Model Name: {{ $labels.model_name }}.
          Affected Pod: {{ $labels.pod }}.

    - alert: VLLMHighP95RequestQueueTime
      expr: |
        histogram_quantile(0.95, sum by (instance, job, namespace, service, model_name, pod, le) (rate(vllm:request_queue_time_seconds_bucket[5m]))) > 1
      for: 5m
      labels:
        severity: warning
        expr: |
          histogram_quantile(0.95, sum by (instance, job, namespace, service, model_name, pod, le) (rate(vllm:request_queue_time_seconds_bucket[5m]))) > 1
        for: 5m
      annotations:
        summary: "High P95 request queue time for vLLM service {{ $labels.service }} in namespace {{ $labels.namespace }}"
        description: |
          The 95th percentile of time requests spend in the queue is consistently above 1 second over a 5-minute window, indicating potential backlog.
          P95 Queue Time: {{ $value | printf "%.2f" }}s
          Model Name: {{ $labels.model_name }}.
          Affected Pod: {{ $labels.pod }}.

    - alert: VLLMHighGPUCacheUsage
      expr: |
        vllm:gpu_cache_usage_perc > 0.9
      for: 10m
      labels:
        severity: warning
        expr: |
          vllm:gpu_cache_usage_perc > 0.9
        for: 10m
      annotations:
        summary: "High GPU cache usage for vLLM service {{ $labels.service }} in namespace {{ $labels.namespace }}"
        description: |
          GPU cache usage exceeds 90%. This indicates potential memory pressure and could lead to performance degradation or OOM errors.
          GPU Cache Usage: {{ $value | printf "%.1f" }}%
          Model Name: {{ $labels.model_name }}.
          Affected Pod: {{ $labels.pod }}.